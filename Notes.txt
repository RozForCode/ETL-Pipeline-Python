What are ETLs and how to create them?
-    Data in organizations is often spread across various sources like spreadsheets, databases, and APIs. A data pipeline consolidates this data into a unified format for analysis. A specific type of data pipeline is ETL (Extract, Transform, Load), which moves data through three key phases:
    Extract: Data is retrieved from sources like spreadsheets, APIs, or databases using tools like Python, AWS Glue, or Azure Data Factory.
    Transform: Data is cleaned, enriched, aggregated, and standardized to remove errors and make it analysis-ready. Tools like Python, Spark, and Azure Data Factory are common.
    Load: Transformed data is loaded into target systems like data warehouses or data lakes for use by analysts or machine learning models. Tools include Python, Spark, and Azure Data Factory.
    Orchestration tools like Apache Airflow and SSIS help schedule, monitor, and manage ETLs, often using Python or Spark to create custom workflows. In this course, Python will be used to build custom ETL workflows, covering all phases of the ETL process.

ETL Overview 
-   Before creating an ETL pipeline, several key factors should be considered to ensure its efficiency and effectiveness. Using the example of a sports apparel company, H+ Sports, here are the considerations:

    Pipeline Architecture and Tools

    Evaluate available tools, required infrastructure, and associated costs.
    Data Sources, Transformations, and Destinations

    Identify data sources (e.g., databases, file systems, APIs) and destinations.
    Define necessary transformations like cleaning, enriching, and aggregating.
    Data Frequency and Volume

    Decide on the frequency of data extraction: incremental vs. full load.
    Full loads are preferred for:
    Data cleaning and refreshing.
    Simplifying schema updates.
    Maintenance simplicity.
    Initial setup or migration.
    Data Volume and Velocity

    Assess the amount of data and its rate of arrival and processing needs.
    Scalability and Performance

    Plan for increased data volume and velocity over time.
    Optimize for performance and scalability to handle future growth.
    Considering these factors ensures the right decisions are made, appropriate tools are chosen, and the ETL pipeline is built to be robust and scalable.

Exploring data with Pandas and SQL
-   The Pandas library is a popular open-source Python library for data manipulation and analysis. It provides easy-to-use functions for working with data and supports two main data structures:

    Series: One-dimensional arrays with labeled data, similar to a column in Excel.
    DataFrames: Two-dimensional tables with rows and columns, similar to Excel spreadsheets or SQL tables.
    To use Pandas:

    Install it using pip and import it into your Python script with import pandas as pd.
    Key Features of Pandas:

    It can import data from various sources, such as CSV, Excel, JSON, SQL databases, etc., using methods like pd.read_csv or pd.read_excel.
    It allows you to explore, clean, and transform data, making it highly versatile for data preparation tasks.
    Pandas is a core tool for building ETL workflows, and further exploration of its capabilities is essential for effective data manipulation.

Understanding Data
-   Exploring data is a crucial step in data warehousing and ETL creation, as it ensures familiarity with the dataset's structure, format, data types, and relationships. Key steps include:

    Reasons for Data Exploration:

    Understand data structure and quality (e.g., duplicates, inconsistencies, missing values).
    Identify required transformations and business logic.
    Determine ETL structuring and standardization needs (e.g., converting time formats).
    Pandas Methods for Data Exploration:

    df.head() and df.tail(): View the first and last rows of the dataset.
    df.shape: Retrieve the number of rows and columns in the dataset.
    df.info(): Summarize data structure, including null values, data types, and memory usage.
    df.duplicated(): Identify duplicate rows, returning a Boolean series.
    Insights from Exploration:

    Sample data includes five rows and seven columns.
    The "time of purchase" column requires transformation to datetime format.
    No duplicate rows were found in the dataset.
    Pandas offers many additional methods for exploring data. 
Loading data from different sources
-   